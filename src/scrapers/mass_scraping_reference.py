#!/usr/bin/env python3
"""
APIF ACTOR - SCRAPING MASSIF E-COMMERCE
=======================================

Actor Apify optimis√© pour le scraping massif multi-plateforme
utilisant les scrapers Amazon et eBay avec techniques anti-d√©tection.

Fonctionnalit√©s:
- Tests de disponibilit√© automatiques
- Gestion d'erreurs robuste avec retry
- Skip automatique des plateformes bloqu√©es
- Statistiques d√©taill√©es et sauvegarde Apify
- Configuration flexible via input schema
- Support Apify Dataset et Key-Value Store

Auteur: Assistant IA
Version: 2.0 (Apify Actor)
"""

import asyncio
import json
import time
import random
import sys
import os
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path

# Import Apify SDK
from apify import Actor

# Ajouter le r√©pertoire racine au path pour les imports
project_root = os.path.join(os.path.dirname(__file__), '..', '..')
sys.path.insert(0, project_root)
# Ajout du r√©pertoire src pour les imports directs
src_path = os.path.join(os.path.dirname(__file__), '..')
sys.path.insert(0, src_path)

# Configuration par d√©faut (sera remplac√©e par les inputs Apify)
DEFAULT_CONFIG = {
    "target_products": 500,
    "max_retries": 3,
    "timeout_per_scraper": 30.0,
    "pause_between_terms": (3.0, 7.0),  # min, max secondes
    "pause_between_retries": (2.0, 5.0),
    "search_terms": [
        "laptop", "gaming laptop", "macbook", "dell laptop", "hp laptop",
        "iphone", "iphone 15", "iphone 14", "smartphone", "apple iphone"
    ],
    "blocked_platforms": [],  # Aucune plateforme bloqu√©e par d√©faut
    "output_dir": "output",
    "results_prefix": "mass_scraping_apify_results"
}

class MassScrapingManager:
    """Gestionnaire principal pour le scraping massif optimis√© avec Apify"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or DEFAULT_CONFIG
        self.scrapers = {}
        self.available_scrapers = []
        self.failed_scrapers = set()
        self.total_products = 0
        self.all_products = []
        self.stats = {
            "start_time": None,
            "end_time": None,
            "duration": 0,
            "total_products": 0,
            "successful_searches": 0,
            "failed_searches": 0,
            "platform_stats": {},
            "term_stats": {}
        }
        
        # Import des scrapers
        self._import_scrapers()
    
    def _import_scrapers(self):
        """Import dynamique des scrapers disponibles"""
        try:
            # Essayer d'importer avec le chemin absolu
            import sys
            import os
            
            # Ajouter le r√©pertoire des scrapers au path
            scrapers_dir = os.path.dirname(__file__)
            if scrapers_dir not in sys.path:
                sys.path.insert(0, scrapers_dir)
            
            from amazon_scraper import AmazonScraper
            from ebay_scraper import EbayScraper
            
            self.scrapers = {
                "amazon": AmazonScraper,
                "ebay": EbayScraper
            }
            print(f"‚úÖ Scrapers import√©s avec succ√®s: {list(self.scrapers.keys())}")
            
        except ImportError as e:
            print(f"‚ö†Ô∏è Erreur d'importation des scrapers: {e}")
            print("üìù Utilisation des MockScrapers pour les tests")
            
            # Fallback vers MockScrapers pour les tests
            class MockScraper:
                async def __aenter__(self):
                    return self
                
                async def __aexit__(self, exc_type, exc_val, exc_tb):
                    pass
                
                async def search_products(self, query: str, max_results: int = 50) -> List[Dict]:
                    # Simuler des produits pour les tests
                    await asyncio.sleep(1)  # Simuler le temps de scraping
                    return [
                        {
                            "title": f"Mock {query} Product {i+1}",
                            "price": f"${(i+1) * 10}.99",
                            "url": f"https://example.com/product-{i+1}",
                            "platform": "mock"
                        }
                        for i in range(min(max_results, 5))  # Limiter √† 5 produits simul√©s
                    ]
            
            self.scrapers = {
                "amazon": MockScraper,
                "ebay": MockScraper
            }
    
    async def test_scraper_availability(self, platform_name: str) -> bool:
        """Test la disponibilit√© d'un scraper avec timeout"""
        print(f"üß™ Test de disponibilit√©: {platform_name}")
        
        # Skip des plateformes bloqu√©es
        if platform_name.lower() in self.config.get("blocked_platforms", []):
            print(f"‚è≠Ô∏è {platform_name} skipp√© (configuration)")
            self.failed_scrapers.add(platform_name)
            return False
        
        try:
            scraper_class = self.scrapers[platform_name]
            
            async with scraper_class(max_results=1) as scraper:
                # Test rapide avec timeout
                test_products = await asyncio.wait_for(
                    scraper.search_products("test"),
                    timeout=self.config.get("timeout_per_scraper", 30.0)
                )
                
                if test_products and len(test_products) > 0:
                    print(f"‚úÖ {platform_name}: Fonctionnel ({len(test_products)} produit(s) test)")
                    return True
                else:
                    print(f"‚ö†Ô∏è {platform_name}: Aucun produit retourn√©")
                    self.failed_scrapers.add(platform_name)
                    return False
                    
        except asyncio.TimeoutError:
            print(f"‚è∞ {platform_name}: Timeout (>{self.config.get('timeout_per_scraper', 30.0)}s)")
            self.failed_scrapers.add(platform_name)
            return False
        except Exception as e:
            print(f"‚ùå {platform_name}: Erreur - {str(e)[:100]}...")
            self.failed_scrapers.add(platform_name)
            return False
    
    async def discover_available_scrapers(self):
        """D√©couvre les scrapers disponibles et fonctionnels"""
        print("\nüß™ PHASE 1: TEST DE DISPONIBILIT√â DES SCRAPERS")
        print("=" * 50)
        
        for platform_name in self.scrapers.keys():
            is_available = await self.test_scraper_availability(platform_name)
            if is_available:
                self.available_scrapers.append(platform_name)
        
        if not self.available_scrapers:
            raise Exception("‚ùå Aucun scraper disponible !")
        
        print(f"\n‚úÖ Scrapers disponibles: {self.available_scrapers}")
    
    async def scrape_term_on_platform(self, term: str, platform: str, attempt: int = 1) -> List[Dict]:
        """Scrape un terme sur une plateforme avec retry"""
        max_retries = self.config.get("max_retries", 3)
        max_results = self.config.get("maxResults", 50)
        
        try:
            print(f"üîç {platform} pour '{term}' (tentative {attempt}/{max_retries})...")
            
            scraper_class = self.scrapers[platform]
            async with scraper_class(max_results=max_results) as scraper:
                products = await asyncio.wait_for(
                    scraper.search_products(term),
                    timeout=self.config.get("timeout_per_scraper", 30.0)
                )
                
                if products:
                    print(f"üì¶ {platform}: {len(products)} produits r√©cup√©r√©s")
                    # Convertir les objets Product en dictionnaires si n√©cessaire
                    products_dict = []
                    for product in products:
                        if hasattr(product, '__dict__'):
                            products_dict.append(product.__dict__)
                        else:
                            products_dict.append(product)
                    return products_dict
                else:
                    print(f"‚ö†Ô∏è {platform}: Aucun produit trouv√©")
                    return []
                    
        except asyncio.TimeoutError:
            print(f"‚è∞ {platform}: Timeout")
            if attempt < max_retries:
                pause_range = self.config.get("pause_between_retries", (2.0, 5.0))
                pause = random.uniform(*pause_range)
                await asyncio.sleep(pause)
                return await self.scrape_term_on_platform(term, platform, attempt + 1)
            return []
            
        except Exception as e:
            print(f"‚ùå {platform}: Erreur - {str(e)[:100]}...")
            if attempt < max_retries:
                pause_range = self.config.get("pause_between_retries", (2.0, 5.0))
                pause = random.uniform(*pause_range)
                await asyncio.sleep(pause)
                return await self.scrape_term_on_platform(term, platform, attempt + 1)
            return []
    
    async def scrape_term_all_platforms(self, term: str) -> List[Dict]:
        """Scrape un terme sur toutes les plateformes disponibles"""
        print(f"\nüöÄ Scraping multi-plateforme pour '{term}'...")
        
        all_products = []
        
        # Scraping s√©quentiel pour √©viter la surcharge
        for platform in self.available_scrapers:
            products = await self.scrape_term_on_platform(term, platform)
            
            # Ajouter m√©tadonn√©es
            for product in products:
                product['search_term'] = term
                product['platform'] = platform
                product['scraped_at'] = datetime.now().isoformat()
            
            all_products.extend(products)
            
            # Mise √† jour des stats
            if platform not in self.stats["platform_stats"]:
                self.stats["platform_stats"][platform] = {"products": 0, "searches": 0, "successes": 0}
            
            self.stats["platform_stats"][platform]["searches"] += 1
            self.stats["platform_stats"][platform]["products"] += len(products)
            if products:
                self.stats["platform_stats"][platform]["successes"] += 1
        
        return all_products
    
    async def run_mass_scraping(self):
        """Ex√©cute le scraping massif principal"""
        self.stats["start_time"] = datetime.now()
        
        print("\nüöÄ PHASE 2: SCRAPING MASSIF")
        print("=" * 50)
        print(f"üìã Termes de recherche: {len(self.config.get('search_terms', []))}")
        print(f"üè™ Plateformes actives: {self.available_scrapers}")
        
        term_count = 0
        cycle_count = 1
        
        while self.total_products < self.config.get("target_products", 500):
            print(f"\nüîÑ CYCLE {cycle_count}")
            print("=" * 60)
            
            for i, term in enumerate(self.config.get("search_terms", [])):
                if self.total_products >= self.config.get("target_products", 500):
                    break
                
                term_count += 1
                print(f"\n{'=' * 60}")
                print(f"üîç TERME {term_count}: '{term}'")
                print("=" * 60)
                
                # Scraping du terme
                products = await self.scrape_term_all_platforms(term)
                
                if products:
                    print(f"‚úÖ Total pour '{term}': {len(products)} produits")
                    self.all_products.extend(products)
                    self.total_products += len(products)
                    print(f"‚úÖ {len(products)} produits ajout√©s (Total: {self.total_products})")
                    
                    # Stats par terme
                    self.stats["term_stats"][term] = self.stats["term_stats"].get(term, 0) + len(products)
                    self.stats["successful_searches"] += 1
                else:
                    print(f"‚ùå Aucun produit pour '{term}'")
                    self.stats["failed_searches"] += 1
                
                # Pause entre termes (sauf si dernier terme)
                search_terms = self.config.get("search_terms", [])
                if i < len(search_terms) - 1 and self.total_products < self.config.get("target_products", 500):
                    pause_range = self.config.get("pause_between_terms", (3.0, 7.0))
                    pause = random.uniform(*pause_range)
                    print(f"‚è≥ Pause inter-terme: {pause:.1f}s...")
                    await asyncio.sleep(pause)
            
            cycle_count += 1
            
            # S√©curit√©: √©viter les boucles infinies
            if cycle_count > 10:
                print("‚ö†Ô∏è Limite de cycles atteinte")
                break
        
        self.stats["end_time"] = datetime.now()
        self.stats["duration"] = (self.stats["end_time"] - self.stats["start_time"]).total_seconds()
        self.stats["total_products"] = self.total_products
    
    def generate_report(self) -> str:
        """G√©n√®re un rapport d√©taill√© des r√©sultats"""
        report = []
        report.append("\n" + "=" * 80)
        report.append("üèÅ TEST TERMIN√â")
        report.append("=" * 80)
        report.append(f"‚è±Ô∏è Dur√©e: {self.stats['duration']:.1f} secondes")
        report.append(f"üì¶ Produits r√©cup√©r√©s: {self.total_products}")
        report.append(f"‚úÖ Recherches r√©ussies: {self.stats['successful_searches']}")
        report.append(f"‚ùå Recherches √©chou√©es: {self.stats['failed_searches']}")
        
        report.append("\n" + "=" * 70)
        report.append("üìä RAPPORT COMPLET DE SCRAPING")
        report.append("=" * 70)
        report.append(f"üì¶ Total produits: {self.total_products}")
        report.append(f"‚úÖ Scrapers fonctionnels: {self.available_scrapers}")
        report.append(f"‚ùå Scrapers d√©faillants: {list(self.failed_scrapers)}")
        
        # Performance des scrapers
        if self.stats["platform_stats"]:
            report.append("\nüéØ Performance des scrapers:")
            for platform, stats in self.stats["platform_stats"].items():
                success_rate = (stats["successes"] / stats["searches"] * 100) if stats["searches"] > 0 else 0
                report.append(f"  ‚Ä¢ {platform}: {stats['successes']}/{stats['searches']} ({success_rate:.1f}% succ√®s)")
        
        # R√©partition par plateforme
        if self.stats["platform_stats"]:
            report.append("\nüè™ R√©partition par plateforme:")
            for platform, stats in self.stats["platform_stats"].items():
                percentage = (stats["products"] / self.total_products * 100) if self.total_products > 0 else 0
                report.append(f"  ‚Ä¢ {platform}: {stats['products']} ({percentage:.1f}%)")
        
        # Top termes
        if self.stats["term_stats"]:
            report.append("\nüîç Top termes de recherche:")
            sorted_terms = sorted(self.stats["term_stats"].items(), key=lambda x: x[1], reverse=True)[:5]
            for term, count in sorted_terms:
                report.append(f"  ‚Ä¢ '{term}': {count} produits")
        
        # Analyse des prix
        if self.all_products:
            prices = []
            for product in self.all_products:
                if 'price' in product and product['price']:
                    try:
                        # Extraction du prix num√©rique
                        price_str = str(product['price']).replace('$', '').replace(',', '')
                        price = float(price_str)
                        if 0 < price < 10000:  # Filtrer les prix aberrants
                            prices.append(price)
                    except:
                        continue
            
            if prices:
                report.append("\nüí∞ Analyse des prix:")
                report.append(f"  ‚Ä¢ Prix minimum: ${min(prices):.2f}")
                report.append(f"  ‚Ä¢ Prix maximum: ${max(prices):.2f}")
                report.append(f"  ‚Ä¢ Prix moyen: ${sum(prices)/len(prices):.2f}")
                report.append(f"  ‚Ä¢ Valeur totale: ${sum(prices):.2f}")
        
        return "\n".join(report)
    
    def save_results(self) -> str:
        """Sauvegarde les r√©sultats en JSON"""
        # Cr√©er le dossier de sortie
        output_dir = Path(CONFIG["output_dir"])
        output_dir.mkdir(exist_ok=True)
        
        # Nom du fichier avec timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{CONFIG['results_prefix']}_{timestamp}.json"
        filepath = output_dir / filename
        
        # Donn√©es √† sauvegarder
        results_data = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "total_products": self.total_products,
                "target_products": CONFIG["target_products"],
                "duration_seconds": self.stats["duration"],
                "available_scrapers": self.available_scrapers,
                "failed_scrapers": list(self.failed_scrapers)
            },
            "statistics": self.stats,
            "products": self.all_products
        }
        
        # Sauvegarde
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ R√©sultats sauvegard√©s: {filename}")
        return str(filepath)
    
    async def run(self):
        """Point d'entr√©e principal"""
        try:
            print("üöÄ SCRIPT DE R√âF√âRENCE - SCRAPING MASSIF APIFY")
            print("=" * 80)
            print(f"üéØ Objectif: {CONFIG['target_products']} produits")
            print(f"üîß Scrapers configur√©s: {list(self.scrapers.keys())}")
            print("=" * 80)
            
            # Phase 1: Tests de disponibilit√©
            await self.discover_available_scrapers()
            
            # Phase 2: Scraping massif
            await self.run_mass_scraping()
            
            # Phase 3: Rapport et sauvegarde
            report = self.generate_report()
            print(report)
            
            filepath = self.save_results()
            
            # R√©sum√© final
            print("\n" + "=" * 80)
            target_products = self.config.get("target_products", 500)
            if self.total_products >= target_products:
                print("üéâ OBJECTIF ATTEINT !")
            else:
                print("‚ö†Ô∏è OBJECTIF PARTIELLEMENT ATTEINT")
            
            print(f"üìä {self.total_products} produits r√©cup√©r√©s (objectif: {target_products})")
            print(f"üìÅ Fichier de r√©sultats: {Path(filepath).name}")
            print("\nüöÄ SCRIPT DE R√âF√âRENCE TERMIN√â !")
            print("‚ö° Version optimis√©e pour Apify Actor")
            
        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è Interruption utilisateur")
            if self.all_products:
                self.save_results()
        except Exception as e:
            print(f"\n‚ùå Erreur critique: {e}")
            if self.all_products:
                self.save_results()


async def main():
    """Fonction principale pour Apify Actor"""
    start_time = time.time()
    
    async with Actor:
        # R√©cup√©rer les inputs Apify
        actor_input = await Actor.get_input() or {}
        
        # Fusionner avec la configuration par d√©faut
        config = {**DEFAULT_CONFIG, **actor_input}
        
        # Cr√©er et ex√©cuter le manager
        manager = MassScrapingManager(config)
        await manager.run()
        
        # Sauvegarder les r√©sultats dans Apify Dataset
        if manager.all_products:
            await Actor.push_data(manager.all_products)
            print(f"‚úÖ {len(manager.all_products)} produits sauvegard√©s dans Apify Dataset")
        
        # Sauvegarder les m√©triques
        metrics = {
            "total_products": manager.total_products,
            "target_products": config.get("target_products", 500),
            "platforms_used": list(manager.available_scrapers),
            "search_terms": config.get("search_terms", []),
            "success_rate": (manager.total_products / config.get("target_products", 500)) * 100 if config.get("target_products", 500) > 0 else 0
        }
        
        await Actor.set_value("METRICS", metrics)
        print(f"üìä M√©triques sauvegard√©es: {metrics}")
        
        # Sauvegarder un r√©sum√© final
        summary = {
            "execution_time": time.time() - start_time,
            "total_products_scraped": manager.total_products,
            "target_achieved": manager.total_products >= config.get("target_products", 500),
            "platforms_tested": len(manager.available_scrapers),
            "search_terms_processed": len(config.get("search_terms", [])),
            "timestamp": datetime.now().isoformat()
        }
        
        await Actor.set_value("EXECUTION_SUMMARY", summary)
        print(f"üéØ R√©sum√© d'ex√©cution sauvegard√©: {summary}")


if __name__ == "__main__":
    asyncio.run(main())